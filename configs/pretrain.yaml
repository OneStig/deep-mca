model:
  hidden_size: 256
  num_layers: 24
  state_size: 16

data:
  dataset: henryc13/x86-pretrain
  split: train[:99%]
  field: instructions
  vocab_path: data/vocab.pkl
  max_seq_len: 256
  
  #can toggle on/off to enable eval during pretraining 
  eval_split: [99%:]

training:
  seed: 42
  batch_size: 8
  lr: 5e-4
  weight_decay: 0.01
  epochs: 5
  warmup_ratio: 0.1
  grad_clip: 1.0
  log_interval: 50

  checkpoint_dir: checkpoints/pretrain

wandb:
  project: deep-mca-pretrain
  entity: mamba-mca
  name: null
