model:
  hidden_size: 256
  num_layers: 24
  state_size: 16

data:
  dataset: "stevenhe04/x86-bb-24m"
  field: "hex"
  max_seq_len: 128
  streaming: true
  shuffle_buffer: 2000

training:
  batch_size: 8
  lr: 5e-4
  weight_decay: 0.01
  max_steps: 4000
  warmup_ratio: 0.1
  log_interval: 50
  save_interval: 2000
  checkpoint_dir: checkpoints/pretrain

wandb:
  project: deep-mca-pretrain
  entity: mamba-mca
  name: null
